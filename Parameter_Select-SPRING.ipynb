{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae811fd5-4787-4233-be74-5d92cfefa486",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kan import *\n",
    "import pytz\n",
    "import time\n",
    "import datetime\n",
    "from data_process import data_process_without_norm\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import copy\n",
    "import pandas as pd\n",
    "\n",
    "df = data_process_without_norm()\n",
    "df = df.drop(\"entsoe\",axis=1)\n",
    "loc_tz = pytz.timezone('Europe/Zurich')\n",
    "split_date_train_ = loc_tz.localize(datetime.datetime(2016,1,1,0,0,0,0))\n",
    "split_date_train = loc_tz.localize(datetime.datetime(2016,3,1,0,0,0,0))\n",
    "split_date_test = loc_tz.localize(datetime.datetime(2016,3,15,0,0,0,0))\n",
    "\n",
    "df_train_ = df.loc[(split_date_train_ < df.index)]\n",
    "df_train = df_train_.loc[df_train_.index <= split_date_train].copy()\n",
    "_temp_df = df.loc[split_date_test > df.index]\n",
    "df_test = _temp_df.loc[_temp_df.index > split_date_train].copy()\n",
    "\n",
    "# Input standardization\n",
    "scaler_input = StandardScaler()\n",
    "# scaler_input = MinMaxScalerr()\n",
    "_temp_scaled_input_data = scaler_input.fit_transform(df_train.iloc[:,1:])\n",
    "x_train_input = _temp_scaled_input_data\n",
    "\n",
    "#Output standardization\n",
    "scaler_output = StandardScaler()\n",
    "# scaler_input = MinMaxScaler()\n",
    "_temp_scaled_output_data = scaler_output.fit_transform(np.array(df_train.iloc[:,0]).reshape(-1,1))\n",
    "x_train_label = _temp_scaled_output_data\n",
    "\n",
    "#Test set standardization\n",
    "x_test_input = scaler_input.transform(df_test.iloc[:,1:])\n",
    "x_test_label = scaler_output.transform(np.array(df_test.iloc[:,0]).reshape(-1,1))\n",
    "\n",
    "_temp_test_input = np.hstack((x_test_input[:,:8],x_test_input[:,-3:-1]))\n",
    "_temp_test_label = x_test_label\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "_temp_train_input = np.hstack((x_train_input[:, :8], x_train_input[:, -3:-1]))\n",
    "_temp_train_label = x_train_label\n",
    "\n",
    "# 将数据转换为 PyTorch 张量\n",
    "train_input_tensor = torch.tensor(_temp_train_input.astype(np.float32))\n",
    "train_label_tensor = torch.tensor(_temp_train_label.astype(np.float32))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fdacfeda-b68c-48e7-9554-5a6045b2e310",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 finish!\n",
      "80 finish!\n",
      "100 finish!\n"
     ]
    }
   ],
   "source": [
    "# description source code annotation\n",
    "# prune_node source code auto_save \n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# 定义 k 折交叉验证\n",
    "k = 3  # 例如，3折交叉验证\n",
    "kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "\n",
    "def evaluate_params(step, lr, grid, k, internal_units, batch, lamb, lamb_entropy, train_input_tensor, train_label_tensor, device):\n",
    "    try:\n",
    "        fold_losses = []\n",
    "        for fold, (train_index, val_index) in enumerate(kf.split(train_input_tensor)):\n",
    "            dataset = dict()\n",
    "            # 获取训练和验证数据\n",
    "            train_input_fold = train_input_tensor[train_index]\n",
    "            train_label_fold = train_label_tensor[train_index]\n",
    "            val_input_fold = train_input_tensor[val_index]\n",
    "            val_label_fold = train_label_tensor[val_index]\n",
    "            dataset['train_input'] = train_input_fold.clone().detach()\n",
    "            dataset['train_label'] = train_label_fold.clone().detach()\n",
    "            dataset['test_input'] = val_input_fold.clone().detach()\n",
    "            dataset['test_label'] = val_label_fold.clone().detach()\n",
    "\n",
    "            model = KAN(width=[10, internal_units, 1], grid=grid, k=k, auto_save=False, seed=0, device=device)\n",
    "            for i in range(internal_units):\n",
    "                model.fix_symbolic(0, 9, i, 'sin', fit_params_bool=False)\n",
    "                model.fix_symbolic(0, 8, i, 'sin', fit_params_bool=False)\n",
    "            try:\n",
    "                model.fit(dataset, opt=\"Adam\", steps=step, batch=batch, lr=lr, lamb=lamb, lamb_entropy=lamb_entropy)\n",
    "                val_predictions = model.forward(dataset['test_input'])\n",
    "                val_loss = mean_squared_error(dataset['test_label'].detach().numpy(), val_predictions.detach().numpy())\n",
    "            except Exception as e:\n",
    "                print(f\"Error during model fitting: {e}\")\n",
    "                val_loss = 100\n",
    "\n",
    "            fold_losses.append(val_loss)\n",
    "\n",
    "        # 计算当前超参数组合在所有折上的平均损失\n",
    "        avg_val_loss = sum(fold_losses) / len(fold_losses)\n",
    "        return {\n",
    "            'step': step,\n",
    "            'lr': lr,\n",
    "            'grid': grid,\n",
    "            'k': k,\n",
    "            'internal_units': internal_units,\n",
    "            'batch': batch,\n",
    "            'lamb': lamb, \n",
    "            'lamb_entropy': lamb_entropy,\n",
    "            'avg_val_loss': avg_val_loss\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error in evaluate_params: {e}\")\n",
    "        return None\n",
    "\n",
    "# 用于存储每个折的性能\n",
    "results_1 = []\n",
    "for step in [50, 80, 100]:\n",
    "    for lr in [0.01, 0.001, 0.0001]:\n",
    "        for grid in [8,10,12]:\n",
    "            for k in [2, 3]:\n",
    "                for lamb in [0.01,0.1]:\n",
    "                    for lamb_entropy in [10,100]:\n",
    "                        for internal_units in [12, 16]:\n",
    "                            for batch in [32, 64]:\n",
    "                                results_1.append(evaluate_params(step, lr, grid, k, internal_units, batch, lamb, lamb_entropy, train_input_tensor, train_label_tensor, device))\n",
    "                #                 break\n",
    "                #             break\n",
    "                #         break\n",
    "                #     break\n",
    "                # break\n",
    "    \n",
    "    print(f'{step} finish!')\n",
    "    results_df = pd.DataFrame(results_1)\n",
    "    results_df.to_csv('./parameters_configuration_1_'+str(step)+'_SPRING.csv')  \n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e252889-d59e-46a4-a49b-706144080b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.to_csv('./parameters_configuration/KAN_BaseConfig_SPRING.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88522ab-e4ab-4907-9a31-3d7e0b71ce74",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kan-kernel",
   "language": "python",
   "name": "kan-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
